{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import os\n",
    "from os.path import join, isfile\n",
    "import random\n",
    "from itertools import groupby\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from experiment import datadir\n",
    "from experimenter.utilities import pop_per_key, sample_per_key, mkdir_p, write_datalist, groupby\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mkdir_p(datadir)\n",
    "base_dir = '/home/bchu/data/sun397'\n",
    "image_dir = join(base_dir, 'images')\n",
    "seed = 1035711226\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "with open(join(base_dir, 'ClassName.txt'), 'r') as f:\n",
    "    categories = [l.strip() for l in f.readlines()]\n",
    "print('Number of categories: %i' % len(categories))\n",
    "def create_label2files(pairs):\n",
    "    key = lambda p: p[1]\n",
    "    return groupby(key, pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label2files = {}\n",
    "train = []\n",
    "for i, category in enumerate(categories):\n",
    "    folder_path = join(base_dir, category[1:]) # strip category's leading slash\n",
    "    files = os.listdir(folder_path)\n",
    "    files = [ (join(folder_path, filename), i) for filename in files]\n",
    "    label2files[i] = files\n",
    "    train.extend(list(files))\n",
    "\n",
    "min_images_per_class = len(min(label2files.items(), key=lambda kv: len(kv[1]))[1])\n",
    "print('Minimum number of train images per class: %i' % min_images_per_class )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = pop_per_key(label2files, 25)\n",
    "val = pop_per_key(label2files, 5)\n",
    "sizes = [1, 10, 50, 70]\n",
    "datasets = []\n",
    "prev_size = 0\n",
    "subset = []\n",
    "for size in sizes:\n",
    "    subset.extend(pop_per_key(label2files, size - prev_size))\n",
    "    datasets.append(subset[:])\n",
    "    prev_size = size\n",
    "sizedatasets = zip(sizes, datasets)\n",
    "print('Test set:', len(test))\n",
    "print('Validation set:', len(val))\n",
    "for size, dataset in sizedatasets:\n",
    "    print('Train %i: %i' % (size, len(dataset)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imagenetdelta_train, imagenetdelta_validate = train_test_split(train, train_size=4500, test_size=500, random_state=seed)\n",
    "imagenetdelta_train = [ (path, 1)  for path, label in imagenetdelta_train ]\n",
    "imagenetdelta_validate = [ (path, 1)  for path, label in imagenetdelta_validate ]\n",
    "imagenetdelta_test = [ (path, 1)  for path, label in random.sample(test, 1000) ]\n",
    "write_datalist(imagenetdelta_train, join(datadir, 'imagenetdelta_train.txt'))\n",
    "write_datalist(imagenetdelta_validate, join(datadir, 'imagenetdelta_validate.txt'))\n",
    "write_datalist(imagenetdelta_test, join(datadir, 'imagenetdelta_test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_datalist(train, join(datadir, 'all.txt'))\n",
    "write_datalist(test, join(datadir, 'test.txt'))\n",
    "write_datalist(val, join(datadir, 'val.txt'))\n",
    "for size, dataset in sizedatasets:\n",
    "    write_datalist(dataset, join(datadir, 'train%i.txt' % size ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def overlap_count(s1, s2):\n",
    "    return len(set(s1) & set(s2))\n",
    "assert overlap_count(val, test) == 0\n",
    "for s in datasets:\n",
    "    assert overlap_count(val, s) == 0\n",
    "    assert overlap_count(test, s) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vallabels = zip(*val)[1]\n",
    "uniq = set(vallabels)\n",
    "plt.hist(vallabels, bins=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
